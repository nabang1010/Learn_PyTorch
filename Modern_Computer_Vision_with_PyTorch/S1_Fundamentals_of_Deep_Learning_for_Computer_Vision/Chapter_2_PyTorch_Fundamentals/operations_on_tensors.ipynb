{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiplication of all the elements present in x by 10 can be performed\n",
    "using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30, 40],\n",
       "        [50, 60, 70, 80]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "x*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding 10 to the elements in x and storing the resulting tensor in y can be\n",
    "performed using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 12, 13, 14],\n",
       "        [15, 16, 17, 18]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "x.add(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reshaping a tensor can be performed using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin y shape torch.Size([4])\n",
      "reshaped y shape torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([2,3,1,0])\n",
    "print(\"origin y shape\", y.shape)\n",
    "y = y.view(4,1)\n",
    "print(\"reshaped y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another way to reshape a tensor is by using the squeeze method, where\n",
    "we provide the axis index that we want to remove. Note that this is\n",
    "applicable only when the axis we want to remove has only one item in that\n",
    "dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,1,10)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = torch.squeeze(x, 1)\n",
    "z1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = x.squeeze(1)\n",
    "z2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The opposite of squeeze is unsqueeze, which means we add a dimension\n",
    "to the matrix, which can be performed using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  torch.randn(10,10)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = x.unsqueeze(0)\n",
    "z1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 10])\n",
      "torch.Size([10, 1, 10])\n",
      "torch.Size([10, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "x  = torch.randn(10,10)\n",
    "z2, z3, z4 = x[None], x[:,None], x[:,:, None]\n",
    "print(z2.shape)\n",
    "print(z3.shape)\n",
    "print(z4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using None for indexing is a fancy way of unsqueezing, as shown,\n",
    "and will be used often in this book for creating fake channel/batch\n",
    "dimensions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Matrix multiplication of two different tensors can be performed using the\n",
    "following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 572,  365],\n",
       "        [1396,  925]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "y = torch.randint(low=1, high=100, size=(4,2))\n",
    "torch.matmul(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Alternatively, matrix multiplication can also be performed by using the @\n",
    "operator:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 572,  365],\n",
       "        [1396,  925]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to concatenate in NumPy, we can perform concatenation of\n",
    "tensors using the cat method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([10, 10, 10])\n",
      "z shape torch.Size([20, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "x =  torch.randn(10,10,10)\n",
    "z = torch.cat([x,x], axis=0)\n",
    "print(\"x shape\", x.shape)\n",
    "print(\"z shape\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([10, 10, 10])\n",
      "z shape torch.Size([10, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x,x], axis=1)\n",
    "print(\"x shape\", x.shape)\n",
    "print(\"z shape\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([10, 10, 10])\n",
      "z shape torch.Size([10, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x,x], axis=2)\n",
    "print(\"x shape\", x.shape)\n",
    "print(\"z shape\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extraction of the maximum value in a tensor can be performed using the\n",
    "following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]])\n",
      "Max torch.Size([5, 5]) tensor(24)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(25).reshape(5,5)\n",
    "print(x)\n",
    "print(\"Max\", x.shape, x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can extract the maximum value along with the row index where the\n",
    "maximum value is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([20, 21, 22, 23, 24]),\n",
       "indices=tensor([4, 4, 4, 4, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that, in the preceding output, we are fetching the maximum values\n",
    "across dimension 0, which is the rows of the tensor. Hence, the maximum\n",
    "values across all rows are the values present in the 4th `index` and hence the\n",
    "indices output is all fours too***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Furthermore, `.max` returns both the\n",
    "maximum values and the location (`argmax`) of the maximum values. Similarly, the output when fetching the maximum value across columns is\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  9, 14, 19, 24]) tensor([4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "m, argm =  x.max(dim=1)\n",
    "print(m, argm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `min` operation is exactly the same as `max` but returns the minimum and\n",
    "arg-minimum where applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Permute the dimensions of a tensor object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([10, 20, 30])\n",
      "z shape torch.Size([30, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "x =  torch.randn(10,20,30)\n",
    "z = x.permute(2,0,1)\n",
    "print(\"x shape\", x.shape)\n",
    "print(\"z shape\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that the shape of the tensor changes when we perform permute on top of the\n",
    "original tensor***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Never reshape (that is, use `tensor.view` on) a tensor to swap the\n",
    "dimensions. Even though Torch will not throw an error, this is\n",
    "wrong and will create unforeseen results during training. If you\n",
    "need to swap dimensions, always use permute***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Standard mathematical operations, such as `abs, add, argsort,\n",
    "ceil, floor, sin, cos, tan, cumsum, cumprod, diag, eig, exp, log, log2, log10,\n",
    "mean, median, mode, resize, round, sigmoid, softmax, square, sqrt, svd,\n",
    "and transpose`, to name a few, can be directly called on any tensor with or without\n",
    "axes where applicable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae33be8d070ec4898e41bd67cea534f6c799a555c8d7c158f20e5f493ed530b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
